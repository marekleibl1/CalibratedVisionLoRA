{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc64302f-6da8-46f5-a700-0c77114de240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples - Train 7520 Test 2528\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Download the dataset\n",
    "\n",
    "n_classes = 10\n",
    "batch_size = 32\n",
    "\n",
    "trainset = torchvision.datasets.Food101(root='../data', split='train', download=True, transform=transform)\n",
    "testset = torchvision.datasets.Food101(root='../data', split='test', download=True, transform=transform)\n",
    "\n",
    "# Select top k most frequent classes\n",
    "# classes, counts = np.unique(trainset._labels, return_counts=True)\n",
    "# idx = np.argsort(counts)[-n_classes:]\n",
    "# target_classes = classes[idx]\n",
    "# print('target_classes', target_classes)\n",
    "\n",
    "target_classes = list(range(n_classes))\n",
    "\n",
    "\n",
    "\n",
    "# Filter the dataset to include only samples from the target classes\n",
    "def filter_classes(dataset, target_classes):\n",
    "    targets = np.array(dataset._labels)\n",
    "    mask = np.isin(targets, target_classes)\n",
    "    indices = np.where(mask)[0]\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "trainset_filtered = filter_classes(trainset, target_classes)\n",
    "testset_filtered = filter_classes(testset, target_classes)\n",
    "\n",
    "# Create a smaller subset for training\n",
    "# subset_indices = torch.randperm(len(dataset)) # [:100]  # Using 100 samples for training\n",
    "# trainset_subset = Subset(dataset, subset_indices)\n",
    "\n",
    "trainloader = DataLoader(trainset_filtered, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset_filtered, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print(f'Training Samples - Train {len(trainloader) * batch_size} Test {len(testloader) * batch_size}')\n",
    "print(f'Batch size: {batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e78c304-aa0b-4f8b-af30-f836d19e5a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n",
      "Total number of parameters: 11181642\n",
      "Number of trainable parameters: 5130\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import math\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, base_layer, rank=4):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.rank = rank\n",
    "        self.lora_A = nn.Parameter(torch.randn(base_layer.weight.size(0), rank))\n",
    "        self.lora_B = nn.Parameter(torch.randn(rank, base_layer.weight.size(1)))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.lora_B, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora_weight = torch.matmul(self.lora_A, self.lora_B)\n",
    "        new_weight = self.base_layer.weight + lora_weight\n",
    "        return nn.functional.linear(x, new_weight, self.base_layer.bias)\n",
    "\n",
    "\n",
    "def apply_lora(model, rank):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            lora_layer = LoRALayer(module, rank)\n",
    "            setattr(model, name, lora_layer)\n",
    "            lora_layer.lora_A.requires_grad = True\n",
    "            lora_layer.lora_B.requires_grad = True\n",
    "    return model\n",
    "    \n",
    "\n",
    "def print_trainable_parameters(model): \n",
    "    # Calculate total number of parameters and trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f'Total number of parameters: {total_params}')\n",
    "    print(f'Number of trainable parameters: {trainable_params}')\n",
    "\n",
    "\n",
    "lora_rank = 1  # when 0, only train the last layer \n",
    "\n",
    "# net = resnet18(pretrained=True)\n",
    "net = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if lora_rank > 0: \n",
    "    net = apply_lora(net, rank=lora_rank)\n",
    "\n",
    "\n",
    "net.fc = nn.Linear(512, n_classes) \n",
    "net.fc.requires_grad = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device', device)\n",
    "net.to(device)\n",
    "\n",
    "\n",
    "print_trainable_parameters(net)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5779ea3-7f09-4100-b510-66973029bde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from os.path import join\n",
    "\n",
    "lr = 0.01 #  0.001 \n",
    "\n",
    "\n",
    "print('Training ...')\n",
    "\n",
    "best_valid_loss = 9999\n",
    "model_dir = './models'\n",
    "export_path = join(model_dir, f'model_lorarank{lora_rank}.pth')\n",
    "\n",
    "net.load_state_dict(torch.load(export_path))\n",
    "net.eval() \n",
    "\n",
    "print('Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018598e5-ad42-4000-bf53-0a82e6be696c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Laplace ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from laplace import Laplace\n",
    "\n",
    "# la = Laplace(\n",
    "#     net,\n",
    "#     likelihood=\"classification\",\n",
    "#     subset_of_weights=\"all\",\n",
    "#     hessian_structure=\"kron\",\n",
    "# )\n",
    "\n",
    "la = Laplace(\n",
    "    net,\n",
    "    likelihood=\"classification\",\n",
    "    subset_of_weights=\"all\",\n",
    "    hessian_structure=\"full\",\n",
    ")\n",
    "\n",
    "print('Fitting Laplace ...')\n",
    "la.fit(trainloader)\n",
    "\n",
    "print('Optimization of HPs,,,')\n",
    "la.optimize_prior_precision()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd315b2-c179-4a42-b61d-3b2b9e66de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f23f4c-9dbe-4b78-81df-9dc677ee17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(dataloader, model, laplace=False):\n",
    "    py = []\n",
    "\n",
    "    for x, _ in dataloader:\n",
    "        if laplace:\n",
    "            py.append(model(x.cuda()))\n",
    "        else:\n",
    "            py.append(torch.softmax(model(x.cuda()), dim=-1))\n",
    "\n",
    "    return torch.cat(py).cpu()\n",
    "\n",
    "\n",
    "probs_laplace = predict(testloader, la, laplace=True)\n",
    "\n",
    "print(probs_laplace)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c68e4-81fa-44dd-84df-259c22c96997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df369b3-0e83-40c3-ae4e-8dd12d23caa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def04000-a155-45fa-905f-2a5c537a22e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.cat([y for x, y in testloader], dim=0).cpu()\n",
    "\n",
    "\n",
    "# print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed33792-8814-4128-b1be-93d1e4742e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netcal.metrics import ECE \n",
    "import torch.distributions as dists\n",
    "\n",
    "# TODO also compare with the baseline that was not finetuned?\n",
    "\n",
    "probs_baseline = predict(testloader, net, laplace=False)\n",
    "\n",
    "\n",
    "def eval(probs, name): \n",
    "    \n",
    "    acc_laplace = (probs.argmax(-1) == targets).float().mean()\n",
    "    ece_laplace = ECE(bins=15).measure(probs.numpy(), targets.numpy())\n",
    "    nll_laplace = -dists.Categorical(probs).log_prob(targets).mean()\n",
    "    \n",
    "    print(\n",
    "        f\"[{name}] Acc.: {acc_laplace:.1%}; ECE: {ece_laplace:.1%}; NLL: {nll_laplace:.3}\"\n",
    "    )\n",
    "\n",
    "eval(probs_baseline, 'Baseline')\n",
    "eval(probs_laplace, 'Laplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364fb123-7e11-410d-9ca7-2d12f858e4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
